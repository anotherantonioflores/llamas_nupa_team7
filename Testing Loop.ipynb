{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032694dd-e92f-449c-90e2-5c9628c4536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment anything if you haven't run it in your environment yet\n",
    "#!pip install ipywidgets\n",
    "#!pip install jupyterlab_widgets\n",
    "#!pip install --upgrade jupyterlab ipywidgets\n",
    "#!pip install --upgrade ipywidgets widgetsnbextension\n",
    "#!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "#!pip install pandas\n",
    "#!pip install evaluate\n",
    "#!pip install bert_score\n",
    "#!pip install rouge_score\n",
    "#!pip install bleu\n",
    "#!pip install pyemd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a03f9484-c08f-435f-adf3-b03d22fafcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "rouge = load('rouge')\n",
    "bleu = load('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26323c7-641a-4415-9efa-629fe6fc4230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f089ac2c92413aa91607db8f7d1545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Bringing in the Llamas3 model\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "\n",
    "    \"text-generation\",\n",
    "\n",
    "    model=model_id,\n",
    "\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "\n",
    "    device=\"cuda\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e52fee1f-1505-4900-b4ae-ec5b2b5ce9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ArrowHuang/NumHG/refs/heads/main/Dataset/fold-1/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "586ef21a-1453-4d92-b555-547387b902f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b8f86fc-2066-48ad-b33c-a678d284af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_answers = []\n",
    "#Creating a loop that asks the same prompt x number of times, gets response, prints response.\n",
    "#This prompt is the baseline prompt, seeing how well the model generates numeral aware headlines, before any kind of training\n",
    "#Also prints the reference headlines\n",
    "#ignore warnings\n",
    "for i in range(num_of_samples):\n",
    "  #  print(df.loc[i,'cloze'])\n",
    "    baseline_prompt = f\"Based on this news article: {df.loc[i, 'text']}, generate a headline. Make sure to include important numbers. Only include your headline in your response. Keep your response to a max of 20 tokens\"\n",
    "\n",
    "    messages = [\n",
    "\n",
    "        {\"role\": \"system\", \"content\": \"You are a journalist tasked to provide headlines for news articles!\"},\n",
    "\n",
    "        {\"role\": \"user\", \"content\": baseline_prompt},\n",
    "\n",
    "    ]\n",
    "    terminators = [\n",
    "\n",
    "        pipe.tokenizer.eos_token_id,\n",
    "\n",
    "        pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "\n",
    "    ]\n",
    "    outputs = pipe(\n",
    "\n",
    "        messages,\n",
    "\n",
    "        max_new_tokens=20,\n",
    "\n",
    "        eos_token_id=terminators,\n",
    "\n",
    "        do_sample=True,\n",
    "\n",
    "        temperature=0.6,\n",
    "\n",
    "        top_p=0.9,\n",
    "        pad_token_id = pipe.tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "    )\n",
    "    assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "   # print(df.loc[i, 'summary'])\n",
    "   # print(assistant_response)\n",
    "    big_answers.append(assistant_response)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "797a74c5-2fe4-4de3-a8cb-11764d590b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = df[0:num_of_samples]\n",
    "reference = reference['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa5ad102-c6ca-4313-acf8-427db86e2b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8668412923812866\n",
      "0.894211345911026\n",
      "0.8801841795444488\n"
     ]
    }
   ],
   "source": [
    "results1 = bertscore.compute(predictions=big_answers, references=reference, lang=\"en\")\n",
    "print(sum(results1['precision'])/len(results1['precision']))\n",
    "print(sum(results1['recall'])/len(results1['recall']))\n",
    "print(sum(results1['f1'])/len(results1['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0436d68-af44-4716-a91b-22acf05e3977",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = rouge.compute(predictions=big_answers, references=reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f84dada9-c6db-4dec-9033-7dffae2cb17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = bleu.compute(predictions=big_answers, references=reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61704da0-b367-4f32-b608-84db92eefae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.34430591077010514,\n",
       " 'rouge2': 0.13897759103641458,\n",
       " 'rougeL': 0.264554448781801,\n",
       " 'rougeLsum': 0.2627033135908826}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7176ba0-3cd6-423a-9516-4f1e37098fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0,\n",
       " 'precisions': [0.21774193548387097,\n",
       "  0.07017543859649122,\n",
       "  0.019230769230769232,\n",
       "  0.0],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.5121951219512195,\n",
       " 'translation_length': 124,\n",
       " 'reference_length': 82}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
