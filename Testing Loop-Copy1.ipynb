{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "032694dd-e92f-449c-90e2-5c9628c4536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment anything if you haven't run it in your environment yet\n",
    "#!pip install ipywidgets\n",
    "#!pip install jupyterlab_widgets\n",
    "#!pip install --upgrade jupyterlab ipywidgets\n",
    "#!pip install --upgrade ipywidgets widgetsnbextension\n",
    "#!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "#!pip install pandas\n",
    "#!pip install evaluate\n",
    "#!pip install bert_score\n",
    "#!pip install rouge_score\n",
    "#!pip install bleu\n",
    "#!pip install pyemd\n",
    "#!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a03f9484-c08f-435f-adf3-b03d22fafcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "#import random\n",
    "#from moverscore_v2 import get_idf_dict, word_mover_score \n",
    "#from collections import defaultdict\n",
    "#bertscore = load(\"bertscore\")\n",
    "#rouge = load('rouge')\n",
    "#bleu = load('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00bab678-95ff-4d2c-b77c-78a7b3f0dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ArrowHuang/NumHG/refs/heads/main/Dataset/fold-1/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c26323c7-641a-4415-9efa-629fe6fc4230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19ceec06bf945d0a31d47fcea433dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/732 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "#Bringing in the Llamas3 model\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "model_id = \"anotherantonioflores/lora_model_prompt4\"\n",
    "\n",
    "model = model_id\n",
    "\n",
    "pipe = pipeline(\n",
    "\n",
    "    \"text-generation\",\n",
    "\n",
    "    model=model_id,\n",
    "\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}#,\n",
    "\n",
    "#    device=\"cuda\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b1d50c8-ab2e-432a-b24e-67b221e7d095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce97c301f194432eb1887e0dad7fb138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a096b5c935c4d74af881c7292c3c68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e1842f1bf54d1c8b6402f788361d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"anotherantonioflores/lora_model_prompt4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190531a-6790-4e75-bf5d-2eb2c7ebc53c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[4, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ac77e03-6c71-40c7-a2d5-0d064af88b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "####RUN THIS\n",
    "alpaca_prompt = \"\"\"\"Based on the following news article, generate one headline. Make sure to include important numbers. Only include your one headline in your response. Keep your response short, with a max of 20 tokens\"\n",
    "\n",
    "### News Article:\n",
    "{}\n",
    "\n",
    "###Response:\n",
    "{}\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6399ea-01a2-4a8c-8e91-906242faa14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####SKIP\n",
    "# alpaca_prompt = Copied from above\n",
    "#`FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = pipe.tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(#np examples\n",
    "       # \"Fred Silverman Put a Series of Hits on All ____ Major Networks\", # input\n",
    "        df.loc[9, 'text'], # instruction\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = pipe.model.generate(**inputs, max_new_tokens = 20, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0319ac0b-47a2-46b3-910d-49a67414aa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pipeline will eliminate some 500 truck trips through the city's narrow, cobblestone alleyways\n"
     ]
    }
   ],
   "source": [
    "####SKIP\n",
    "text = tokenizer.batch_decode(outputs)\n",
    "text_string = text[0]\n",
    "# Find everything after \"###Response:\"\n",
    "match = re.search(r'(?<=###Response:\\s)(.*)', text_string, re.DOTALL)\n",
    "\n",
    "keyphrase = match.group(1)\n",
    "keyphrase = keyphrase.replace(\"<|end_of_text|>\", \"\")\n",
    "\n",
    "print(keyphrase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fade31a2-0629-4187-a330-169dc5313dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_number = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04bc86d5-032d-421c-b4cd-97915e4b6b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cycle\n",
      "100 cycle\n",
      "200 cycle\n",
      "300 cycle\n",
      "400 cycle\n",
      "500 cycle\n",
      "600 cycle\n",
      "700 cycle\n",
      "800 cycle\n",
      "900 cycle\n"
     ]
    }
   ],
   "source": [
    "#NEW LOOOOP\n",
    "big_answers = []\n",
    "\n",
    "\n",
    "for i in range(stop_number):\n",
    "    inputs = pipe.tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(#np examples\n",
    "           # \"Fred Silverman Put a Series of Hits on All ____ Major Networks\", # input\n",
    "            df.loc[i, 'text'], # instruction\n",
    "            \"\", # output - leave this blank for generation!\n",
    "        )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = pipe.model.generate(**inputs, max_new_tokens = 20, use_cache = True)\n",
    "    tokenizer.batch_decode(outputs)\n",
    "\n",
    "    \n",
    "    text = tokenizer.batch_decode(outputs)\n",
    "    text_string = text[0]\n",
    "    # Find everything after \"###Response:\"\n",
    "    match = re.search(r'(?<=###Response:\\s)(.*)', text_string, re.DOTALL)\n",
    "\n",
    "    keyphrase = match.group(1)\n",
    "    keyphrase = keyphrase.replace(\"<|end_of_text|>\", \"\")\n",
    "\n",
    "    #print(i, keyphrase)\n",
    "    big_answers.append(keyphrase)\n",
    "    if i % 100 == 0:\n",
    "        print(i, \"cycle\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b1c9959-98e8-4160-a352-7459fb286271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(big_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc46a1-bac5-466d-a80e-d4c1f86e2a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(big_answers)):\n",
    "    print(i, big_answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c1fac8b-90f6-4c26-a164-0237f34cfd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(big_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93e6e7f6-3224-4642-8a25-c5032aa8c932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"The Greatest US Credit Card Data Theft Ever\"\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"New York City Abortion Rate Rises to 41%\"\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*Turkey earthquake death toll could be 1,000*\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iran is still four years away from developing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Moriarty's disappearance is a mystery that's b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  \"The Greatest US Credit Card Data Theft Ever\"\\...\n",
       "1  \"New York City Abortion Rate Rises to 41%\"\\n\\n...\n",
       "2    *Turkey earthquake death toll could be 1,000*\\n\n",
       "3  Iran is still four years away from developing ...\n",
       "4  Moriarty's disappearance is a mystery that's b..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2be1c985-576b-4bfb-ab7e-d47e5ca4f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE ALL N\\\n",
    "for i in range(len(output_df)):\n",
    "    if \"\\n\" in output_df.loc[i, 0]:\n",
    "        output_df.loc[i,0] = output_df.loc[i,0].replace(\"\\n\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8671e653-5cde-4a5a-b9fc-f31ca067af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST FOR \\n\n",
    "\n",
    "for i in range(len(output_df)):\n",
    "    if \"\\n\" in output_df.loc[i, 0]:\n",
    "        print(i, \"time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb2f41fb-650a-4535-9703-d7689c6f0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv('model_prompt4_preds_1k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f86fc-2066-48ad-b33c-a678d284af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###OLD MODEL\n",
    "\n",
    "\n",
    "big_answers = []\n",
    "#Creating a loop that asks the same prompt x number of times, gets response, prints response.\n",
    "#This prompt is the baseline prompt, seeing how well the model generates numeral aware headlines, before any kind of training\n",
    "#Also prints the reference headlines\n",
    "#ignore warnings\n",
    "for i in range(num_of_samples):\n",
    "  #  print(df.loc[i,'cloze'])\n",
    "    baseline_prompt = f\"Based on this news article: {df.loc[i, 'text']}, generate a headline. Make sure to include important numbers. Only include your headline in your response. Keep your response to a max of 20 tokens\"\n",
    "\n",
    "    messages = [\n",
    "\n",
    "        {\"role\": \"system\", \"content\": \"You are a journalist tasked to provide headlines for news articles!\"},\n",
    "\n",
    "        {\"role\": \"user\", \"content\": baseline_prompt},\n",
    "\n",
    "    ]\n",
    "    terminators = [\n",
    "\n",
    "        pipe.tokenizer.eos_token_id,\n",
    "\n",
    "        pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "\n",
    "    ]\n",
    "    outputs = pipe(\n",
    "\n",
    "        messages,\n",
    "\n",
    "        max_new_tokens=20,\n",
    "\n",
    "        eos_token_id=terminators,\n",
    "\n",
    "        do_sample=True,\n",
    "\n",
    "        temperature=0.6,\n",
    "\n",
    "        top_p=0.9,\n",
    "        pad_token_id = pipe.tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "    )\n",
    "    assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "   # print(df.loc[i, 'summary'])\n",
    "   # print(assistant_response)\n",
    "    big_answers.append(assistant_response)\n",
    "    print(i)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a65bf-5954-44d0-9c80-ff71e81c64d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(big_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797a74c5-2fe4-4de3-a8cb-11764d590b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = df[0:num_of_samples]\n",
    "reference = reference['summary']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
