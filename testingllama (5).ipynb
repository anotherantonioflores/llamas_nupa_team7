{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce7bcfce-8d75-42be-8642-77a0c625b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment anything if you haven't run it in your environment yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e396f24b-1b82-4127-af35-5a61b5aa6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe2c399d-d07e-4a49-93b5-20bc067880ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install jupyterlab_widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf994d17-902d-48fc-9d38-ad66e574b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade jupyterlab ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772eef1a-bd9a-4e4d-8f08-dac8f0348111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade ipywidgets widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69526b78-dc71-4782-879f-9da1b934925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7865561-ad51-4db6-bc3c-6319ed07aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa26edc-0615-4159-b490-951f894d1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate\n",
    "#!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ca485a8-cd9c-4a4f-a879-6adda90e5b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "rouge = load('rouge')\n",
    "bleu = load('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb6c005-b851-4acc-8931-455cc5a4f1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e1959710fa4f248ee537dde92084a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Bringing in the Llamas3 model\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "\n",
    "    \"text-generation\",\n",
    "\n",
    "    model=model_id,\n",
    "\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "\n",
    "    device=\"cuda\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d711ffb-1c86-4a94-aa3a-0c6183314548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing basic prompt functionality\n",
    "news = \"At least 30 gunmen burst into a drug rehabilitation center in a Mexican border state capital and opened fire, killing 19 men and wounding four people, police said. Gunmen also killed 16 people in another drug-plagued northern city. The killings in Chihuahua city and in Ciudad Madero marked one of the bloodiest weeks ever in Mexico and came just weeks after authorities discovered 55 bodies in an abandoned silver mine, presumably victims of the country’s drug violence. More than 60 people have died in mass shootings at rehab clinics in a little less than two years. Police have said two of Mexico’s six major drug cartels are exploiting the centers to recruit hit men and drug smugglers, ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c11fded-8cca-4dbc-840d-71c4e1f73a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"Generate five different headlines for this news article: {news}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a787a9a9-8bcf-470a-a266-47d00e10c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "\n",
    "    {\"role\": \"system\", \"content\": \"You are a journalist tasked to provide headlines for news articles!\"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f5565a-4cce-42a0-ba86-096c4a2c097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "\n",
    "    pipe.tokenizer.eos_token_id,\n",
    "\n",
    "    pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "883a1d42-2d84-49fc-8cab-3ae9d93998ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "\n",
    "    messages,\n",
    "\n",
    "    max_new_tokens=256,\n",
    "\n",
    "    eos_token_id=terminators,\n",
    "\n",
    "    do_sample=True,\n",
    "\n",
    "    temperature=0.6,\n",
    "\n",
    "    top_p=0.9,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b4258e8-9148-44a1-977b-745f04872751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are five different headline options for the news article:\n",
      "\n",
      "1. \"Bloodbath in Mexico: 19 Killed, 4 Wounded in Mass Shooting at Drug Rehab Center\"\n",
      "2. \"Drug Violence Erupts in Mexico: 35 Killed in Two Cities, Including 19 at Rehab Center\"\n",
      "3. \"Mexican Border State Capital Rocked by Massacre at Drug Rehab Center\"\n",
      "4. \"Mexico's Bloody Week: 35 Killed in Drug-Related Violence, Including 19 at Rehab Center\"\n",
      "5. \"Drug Cartels Target Mexico's Rehab Centers: 35 Killed in Two Cities in Latest Violence\"\n"
     ]
    }
   ],
   "source": [
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "babcb7d9-68f8-41a9-983a-fc2e190958bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in raw data for fold-1 from NumHG dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ArrowHuang/NumHG/refs/heads/main/Dataset/fold-1/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e31974f8-78f1-402e-9031-d6550e2ef68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>cloze</th>\n",
       "      <th>cloze_gt</th>\n",
       "      <th>cloze_annotation</th>\n",
       "      <th>need_reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Jan 30, 2020  7:00 PM) Fred Silverman, who st...</td>\n",
       "      <td>Fred Silverman Put a Series of Hits on All 3 M...</td>\n",
       "      <td>Fred Silverman Put a Series of Hits on All ___...</td>\n",
       "      <td>3</td>\n",
       "      <td>Trans( Three )</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Mar 8, 2008  5:17 PM) Barack Obama easily won...</td>\n",
       "      <td>Obama Wins 7 Delegates in Wyo.</td>\n",
       "      <td>Obama Wins ____ Delegates in Wyo.</td>\n",
       "      <td>7</td>\n",
       "      <td>Copy( 7 )</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Mar 12, 2014  6:32 PM CDT) They may be small,...</td>\n",
       "      <td>Scholar Finds 9 More Dead Sea Scrolls</td>\n",
       "      <td>Scholar Finds ____ More Dead Sea Scrolls</td>\n",
       "      <td>9</td>\n",
       "      <td>Trans( nine )</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Apr 14, 2019  6:07 AM CDT) Babies wail as a n...</td>\n",
       "      <td>On Island Nation, Measles Has Killed 1.2K</td>\n",
       "      <td>On Island Nation, Measles Has Killed ____K</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Paraphrase( 1,200 , K )</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Feb 26, 2015  8:11 AM) Italian surgeon Sergio...</td>\n",
       "      <td>Surgeon: We Could Transplant Human Head in 2017</td>\n",
       "      <td>Surgeon: We Could Transplant Human Head in ____</td>\n",
       "      <td>2017</td>\n",
       "      <td>Copy( 2017 )</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  (Jan 30, 2020  7:00 PM) Fred Silverman, who st...   \n",
       "1  (Mar 8, 2008  5:17 PM) Barack Obama easily won...   \n",
       "2  (Mar 12, 2014  6:32 PM CDT) They may be small,...   \n",
       "3  (Apr 14, 2019  6:07 AM CDT) Babies wail as a n...   \n",
       "4  (Feb 26, 2015  8:11 AM) Italian surgeon Sergio...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Fred Silverman Put a Series of Hits on All 3 M...   \n",
       "1                     Obama Wins 7 Delegates in Wyo.   \n",
       "2              Scholar Finds 9 More Dead Sea Scrolls   \n",
       "3          On Island Nation, Measles Has Killed 1.2K   \n",
       "4    Surgeon: We Could Transplant Human Head in 2017   \n",
       "\n",
       "                                               cloze cloze_gt  \\\n",
       "0  Fred Silverman Put a Series of Hits on All ___...        3   \n",
       "1                  Obama Wins ____ Delegates in Wyo.        7   \n",
       "2           Scholar Finds ____ More Dead Sea Scrolls        9   \n",
       "3         On Island Nation, Measles Has Killed ____K      1.2   \n",
       "4    Surgeon: We Could Transplant Human Head in ____     2017   \n",
       "\n",
       "          cloze_annotation  need_reasoning  \n",
       "0           Trans( Three )               1  \n",
       "1                Copy( 7 )               0  \n",
       "2            Trans( nine )               1  \n",
       "3  Paraphrase( 1,200 , K )               1  \n",
       "4             Copy( 2017 )               0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9eeb05ca-bb2e-4ede-9bd7-60d5748f8d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                (Jan 30, 2020  7:00 PM) Fred Silverman, who st...\n",
       "summary             Fred Silverman Put a Series of Hits on All 3 M...\n",
       "cloze               Fred Silverman Put a Series of Hits on All ___...\n",
       "cloze_gt                                                            3\n",
       "cloze_annotation                                       Trans( Three )\n",
       "need_reasoning                                                      1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Access first row\n",
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a014f298-5d87-4d08-94fb-a2a188283db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fred Silverman Put a Series of Hits on All 3 Major Networks'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Access individual cells\n",
    "df.loc[0, 'summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eeb8043d-91d3-4e0d-a795-1a8617ca6c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(Jan 30, 2020  7:00 PM) Fred Silverman, who steered programming for each of the Big Three broadcast networks and brought All in the Family, Roots, Hawaii Five-O and other hit series and miniseries to television, died Thursday. He was 82. Silverman, who had been battling cancer, died at his home in the Pacific Palisades area of Los Angeles, the AP reports. Silverman's gift for picking winners prompted Time magazine to dub him  The Man with the Golden Gut  in a 1977 profile. As ABC's entertainment chief, Silverman turned the network’s fortunes around with shows including Roots, Rich Man, Poor Man and Charlie’s Angels. He had already brought success to CBS with an overhaul that included replacing country-themed series such as Green Acres with what advertisers considered more upscale and urban fare, including The Mary Tyler Moore Show, The Bob Newhart Show and Mannix. He couldn't repeat that success when he moved to NBC.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db0ea504-8175-4eb8-91a8-97c449ca7f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fred Silverman Put a Series of Hits on All ____ Major Networks'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{df.loc[0, 'cloze']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d9256ba-dfcc-48e0-a9a2-62c96127a982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "#Possible prompt structure for filling in blank prompt (Task 2?)\n",
    "\n",
    "prompt = f\"Based on this news article: {df.loc[0, 'text']}, fill in the blank of this sentence with one or more digits: {df.loc[0, 'cloze']}.Do not put a word in the blank, instead fill it with a digit betwen 1 and 10\"\n",
    "messages = [\n",
    "\n",
    "    {\"role\": \"system\", \"content\": \"You are a journalist tasked to provide headlines for news articles!\"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "\n",
    "]\n",
    "terminators = [\n",
    "\n",
    "    pipe.tokenizer.eos_token_id,\n",
    "\n",
    "    pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "\n",
    "]\n",
    "outputs = pipe(\n",
    "\n",
    "    messages,\n",
    "\n",
    "    max_new_tokens=256,\n",
    "\n",
    "    eos_token_id=terminators,\n",
    "\n",
    "    do_sample=True,\n",
    "\n",
    "    temperature=0.6,\n",
    "\n",
    "    top_p=0.9,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8ffa9d2-b132-4d97-8509-dfc094a13b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the completed sentence:\n",
      "\n",
      "Fred Silverman Put a Series of Hits on All 3 Major Networks.\n"
     ]
    }
   ],
   "source": [
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "542c04a0-b8a4-49f8-9e2d-b3fe34350104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "#Testing out Bertscore\n",
    "#ignore warnings\n",
    "predictions = [\"TV Programming Legend Fred Silverman Dies at 82: 'Man with the Golden Gut' Leaves Lasting Impact on Small Screen\"]\n",
    "references = [\"Fred Silverman Put a Series of Hits on All 3 Major Networks\"]\n",
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79ef32da-7eec-43a6-8a0c-f127014f95db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': [0.8346578478813171],\n",
       " 'recall': [0.864138126373291],\n",
       " 'f1': [0.8491421341896057],\n",
       " 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.46.1)'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a15a3a-61ee-415a-9b65-d24df4718e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get just op five rows\n",
    "top_five = df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4d2a912-b72b-410c-abeb-0679ba72bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating list to hold answers generated by the model\n",
    "answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "879b45b8-33ee-4b91-857f-548370bf8fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fred Silverman Put a Series of Hits on All 3 Major Networks\n",
      "\"Fred Silverman, TV Programming Legend, Dies at 82\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama Wins 7 Delegates in Wyo.\n",
      "\"Obama Sweeps Wyoming Caucuses, Wins 7 Delegates to Clinton's 5\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scholar Finds 9 More Dead Sea Scrolls\n",
      "\"Nine Tiny Scrolls Uncovered at Israel Museum, Secrets Unraveled\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Island Nation, Measles Has Killed 1.2K\n",
      "\"Madagascar's Measles Epidemic: 115,000 Cases, 1,200\n",
      "Surgeon: We Could Transplant Human Head in 2017\n",
      "\"Head Transplant by 2017: Italian Surgeon Unveils Controversial Plan\"\n"
     ]
    }
   ],
   "source": [
    "#Creating a loop that asks the same prompt x number of times, gets response, prints response.\n",
    "#This prompt is the baseline prompt, seeing how well the model generates numeral aware headlines, before any kind of training\n",
    "#Also prints the reference headlines\n",
    "#ignore warnings\n",
    "for i in range(len(top_five)):\n",
    "  #  print(df.loc[i,'cloze'])\n",
    "    baseline_prompt = f\"Based on this news article: {df.loc[i, 'text']}, generate a headline. Make sure to include important numbers. Only include your headline in your response. Keep your response to a max of 20 tokens\"\n",
    "\n",
    "    messages = [\n",
    "\n",
    "        {\"role\": \"system\", \"content\": \"You are a journalist tasked to provide headlines for news articles!\"},\n",
    "\n",
    "        {\"role\": \"user\", \"content\": baseline_prompt},\n",
    "\n",
    "    ]\n",
    "    terminators = [\n",
    "\n",
    "        pipe.tokenizer.eos_token_id,\n",
    "\n",
    "        pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "\n",
    "    ]\n",
    "    outputs = pipe(\n",
    "\n",
    "        messages,\n",
    "\n",
    "        max_new_tokens=20,\n",
    "\n",
    "        eos_token_id=terminators,\n",
    "\n",
    "        do_sample=True,\n",
    "\n",
    "        temperature=0.6,\n",
    "\n",
    "        top_p=0.9,\n",
    "\n",
    "    )\n",
    "    assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "    print(df.loc[i, 'summary'])\n",
    "    print(assistant_response)\n",
    "    answers.append(assistant_response)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "778e8e44-c4b1-483c-9e3a-d29f352eb3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Fred Silverman, TV Programming Legend, Dies at 82\"',\n",
       " '\"Obama Wins Wyoming Caucuses with 61%, Adds 7 Delegates\"',\n",
       " '\"Nine Tiny Scrolls Uncovered at Israel Museum, Mystery Unraveled\"',\n",
       " '\"Madagascar\\'s Measles Outbreak Claims 1,200 Lives, 115,000',\n",
       " '\"Head Transplant Possible by 2017, Italian Surgeon Claims\"']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0aaee40-4604-4875-aa8a-31f6cf60e7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Fred Silverman Put a Series of Hits on All 3 M...\n",
       "1                       Obama Wins 7 Delegates in Wyo.\n",
       "2                Scholar Finds 9 More Dead Sea Scrolls\n",
       "3            On Island Nation, Measles Has Killed 1.2K\n",
       "4      Surgeon: We Could Transplant Human Head in 2017\n",
       "Name: summary, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = top_five['summary']\n",
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce906603-ac44-4f92-a94b-ae5192ed013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "results = bertscore.compute(predictions=answers, references=reference, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "414f2a36-b569-4ab8-84cd-b422947f0d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': [0.8543403744697571,\n",
       "  0.8721169233322144,\n",
       "  0.8609298467636108,\n",
       "  0.8704819083213806,\n",
       "  0.8639395236968994],\n",
       " 'recall': [0.8551610708236694,\n",
       "  0.9238452315330505,\n",
       "  0.8928732872009277,\n",
       "  0.8812599182128906,\n",
       "  0.8999699354171753],\n",
       " 'f1': [0.8547505140304565,\n",
       "  0.897236168384552,\n",
       "  0.8766106367111206,\n",
       "  0.8758377432823181,\n",
       "  0.8815867304801941],\n",
       " 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.46.1)'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10973de0-f22f-47ee-ab19-27a7d929c45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8643617153167724\n",
      "0.8906218886375428\n",
      "0.8772043585777283\n"
     ]
    }
   ],
   "source": [
    "#Prints the average precision, recall, and f1 score\n",
    "print(sum(results['precision'])/len(results['precision']))\n",
    "print(sum(results['recall'])/len(results['recall']))\n",
    "print(sum(results['f1'])/len(results['f1']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cff49427-58bd-4472-84a5-e57a03539108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71036598-687b-4208-aaf4-c24f00287f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e121b45-19e6-4307-b17b-8a2ef53cef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rouge.compute(predictions=answers, references=reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ca8b3a0-18b5-41f7-9f95-3e2e25e50789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': np.float64(0.3002941176470588),\n",
       " 'rouge2': np.float64(0.07555555555555556),\n",
       " 'rougeL': np.float64(0.2532352941176471),\n",
       " 'rougeLsum': np.float64(0.24970588235294117)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "512594b6-5983-4510-98b5-347d77be06fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62a7a7fa-ee4a-44ed-9983-1c22b0b1144d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d3c114d1ef4bc6bfc05e4555b067a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08146c12ff9b417092b0cc7a4b8460b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e6a5ec8f0b477593f1cc856b3e4e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bleu = load('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85e6aa07-d7b9-44fd-a0cb-e1fc3c4b8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = bleu.compute(predictions=answers, references=reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb3fe273-7cf1-46a0-b419-ffdde2f86170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0,\n",
       " 'precisions': [0.2413793103448276,\n",
       "  0.05660377358490566,\n",
       "  0.020833333333333332,\n",
       "  0.0],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.3488372093023255,\n",
       " 'translation_length': 58,\n",
       " 'reference_length': 43}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cda27a2-7ee0-4a60-951e-a7951a788525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyemd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964e2bf-3150-4de2-9d72-e9239adcb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_answers = []\n",
    "#Creating a loop that asks the same prompt x number of times, gets response, prints response.\n",
    "#This prompt is the baseline prompt, seeing how well the model generates numeral aware headlines, before any kind of training\n",
    "#Also prints the reference headlines\n",
    "#ignore warnings\n",
    "for i in range(1000):\n",
    "  #  print(df.loc[i,'cloze'])\n",
    "    baseline_prompt = f\"Based on this news article: {df.loc[i, 'text']}, generate a headline. Make sure to include important numbers. Only include your headline in your response. Keep your response to a max of 20 tokens\"\n",
    "\n",
    "    messages = [\n",
    "\n",
    "        {\"role\": \"system\", \"content\": \"You are a journalist tasked to provide headlines for news articles!\"},\n",
    "\n",
    "        {\"role\": \"user\", \"content\": baseline_prompt},\n",
    "\n",
    "    ]\n",
    "    terminators = [\n",
    "\n",
    "        pipe.tokenizer.eos_token_id,\n",
    "\n",
    "        pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "\n",
    "    ]\n",
    "    outputs = pipe(\n",
    "\n",
    "        messages,\n",
    "\n",
    "        max_new_tokens=20,\n",
    "\n",
    "        eos_token_id=terminators,\n",
    "\n",
    "        do_sample=True,\n",
    "\n",
    "        temperature=0.6,\n",
    "\n",
    "        top_p=0.9,\n",
    "\n",
    "    )\n",
    "    assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "   # print(df.loc[i, 'summary'])\n",
    "   # print(assistant_response)\n",
    "    big_answers.append(assistant_response)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6780b5f4-232f-4532-9b4d-81de63bc3df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = df[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67c5207f-0942-4a45-bf2b-fa5b00d2a153",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = reference['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00621a9b-3828-4a89-81ca-6a3be94daae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "results1 = bertscore.compute(predictions=big_answers, references=reference, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d950ccc7-9656-4650-88b5-6c5a6e5c0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd228dae-0ed5-4ec1-bc79-9da156cb0e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.865951373398304\n",
      "0.893911140859127\n",
      "0.8795973345041275\n"
     ]
    }
   ],
   "source": [
    "#Prints the average precision, recall, and f1 score\n",
    "print(sum(results1['precision'])/len(results1['precision']))\n",
    "print(sum(results1['recall'])/len(results1['recall']))\n",
    "print(sum(results1['f1'])/len(results1['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5859461-dc5d-4fc8-8703-e62d281f8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = rouge.compute(predictions=big_answers, references=reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a59deb05-9840-4dee-9cb4-5ae2c07697ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.3396755451430714,\n",
       " 'rouge2': 0.125345822013051,\n",
       " 'rougeL': 0.2976609741214973,\n",
       " 'rougeLsum': 0.29743337888042815}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0d905cd-5bbb-465d-9b74-4b769a9e1f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = bleu.compute(predictions=big_answers, references=reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "007111f7-6ac8-48cd-9cc8-8ec2eff983e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0529367301700865,\n",
       " 'precisions': [0.2365124194365993,\n",
       "  0.08283238348036537,\n",
       "  0.03209416546701668,\n",
       "  0.012489592006661115],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.6353623874940786,\n",
       " 'translation_length': 13809,\n",
       " 'reference_length': 8444}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8657d5c8-2961-48d2-b541-f0380078f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gabby's BERTScore Code\n",
    "#from bert_score import score\n",
    "\n",
    "# Sample candidate and reference sentences\n",
    "#candidates = [\n",
    "#    \"The cat is on the mat.\",\n",
    "#    \"There is a cat lying on a mat.\"\n",
    "#]\n",
    "#references = [\n",
    "#    \"The cat is sitting on the mat.\",\n",
    "#    \"A cat is on the mat.\"\n",
    "#]\n",
    "\n",
    "# Compute BERTScore\n",
    "#P, R, F1 = score(candidates, references, lang=\"en\", verbose=True)\n",
    "\n",
    "# Print the results\n",
    "#for i, (precision, recall, f1) in enumerate(zip(P, R, F1)):\n",
    "#    print(f\"Sentence {i+1}:\")\n",
    "#    print(f\"  Precision: {precision:.4f}\")\n",
    "#    print(f\"  Recall: {recall:.4f}\")\n",
    "#    print(f\"  F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
